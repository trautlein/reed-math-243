%Header
    \documentclass[11pt]{article}
    \usepackage{indentfirst}
    \usepackage{fullpage}
    \usepackage{microtype}
    \usepackage{enumerate}
    \usepackage{amsmath}
    \usepackage{courier}

    \begin{document}    
    
\noindent{\large
    Hans Trautlein          \hfill Problem Set 2: Chapter 3\ \ \\
    Due: February 5, 2016   \hfill Andrew Bray}
\bigskip

\section*{Book Problems}


\begin{enumerate}
	\item \textit{Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of \texttt{sales}, \texttt{TV}, \texttt{radio}, and \texttt{newspaper}, rather than in terms of the coefficients of the linear model.}

    \begin{description}
        \item[TV] The null hypothesis here is that spending on television advertising didn't affect sales. The p-value here is very small, so we can reject the null hypothesis at a significant level of $\alpha=0.05.$ This means that television advertising does have an affect on sales. 

        \item[Radio] The null hypothesis here is that spending on radio advertising didn't affect sales. The p-value here is very small, so we can reject the null hypothesis at a significant level of $\alpha=0.05.$ This means that radio advertising does have an affect on sales. 

        \item[Newspaper] The null hypothesis here is that spending on newspaper advertising doesn't affect sales. The p-value, at \texttt{0.8599} means that we fail to reject the null hypothesis, which in this circumstance implies that newspaper advertising doesn't affect sales.
         
    \end{description}
	
  	\setcounter{enumi}{4}
	\item \textit{Consider the fitted values that result from performing linear regression without an intercept. In this setting, the $i$th fitted value takes the form}
	
	$$ \hat{y}_i = x_i\hat{\beta},$$
	
	\textit{where}
	
	$$ \hat{\beta} = \left(\sum_{i=1}^nx_iy_i\right) / \left(\sum_{i'=1}^nx_{i'}^2\right).$$
	
	%\sum_{i=1}^nx_iy_i \right  \left \sum_{i=1}^nx_{i'}^2
	
	\textit{Show that we can write}
	
	$$ \hat{y}_i = \sum_{i'=1}^na_{i'}y_{i'}.$$
	
	\textit{What is $a_{i'}$?}
	
	\textit{Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.}
	
	\item \textit{Using (3.4), argue that in the case of simple linear regression, the
least squares line always passes through the point (x?, y?).
}
	
\end{enumerate}


\section*{Challenge Problem}
Use the identities for expected value and variance to derive the bias-variance decomposition of 

$$ E\left[ \left(y - \hat{f}(x) \right)^2 \right].$$

$E(c) = c,$ where $c$ is constant.

$Var(X) = E(X^2) - [E(X)]^2$ 

\end{document}

